#!/usr/bin/env python3
"""
é€‰æ‹©å™¨è®­ç»ƒå™¨
ç¦»çº¿è®­ç»ƒå’Œåœ¨çº¿æ›´æ–°é€‰æ‹©å™¨çŠ¶æ€
"""

import pandas as pd
import numpy as np
import argparse
import os
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from strategies.library import get_strategy_library
from strategies.middleware import apply_middleware, calculate_trade_costs
from selector.context_features import build_regime_features
from selector.thompson_selector import create_selector, ThompsonSelectorConfig

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    train_start: str
    train_end: str
    selector_type: str = 'thompson'
    topk: int = 1
    save_path: str = 'artifacts/selector_state.pkl'
    data_path: str = './data'
    assets: List[str] = None
    reward_function: str = 'sharpe'  # 'sharpe', 'return', 'risk_adjusted'
    alpha_dd: float = 0.5  # å›æ’¤æƒ©ç½šç³»æ•°
    alpha_vol: float = 0.3  # æ³¢åŠ¨ç‡æƒ©ç½šç³»æ•°

class SelectorTrainer:
    """é€‰æ‹©å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.strategy_library = get_strategy_library()
        
        # åˆ›å»ºartifactsç›®å½•
        os.makedirs('artifacts', exist_ok=True)
        
        # é»˜è®¤èµ„äº§åˆ—è¡¨
        if config.assets is None:
            config.assets = ['BTC', 'ETH', 'SOL', 'ARB', 'LINK', 'AAVE', 'LDO', 'FET', 'XLM']
    
    def load_asset_data(self, asset: str) -> pd.DataFrame:
        """åŠ è½½èµ„äº§æ•°æ®"""
        csv_path = os.path.join(self.config.data_path, f"stellar_ohlc_15m_{asset}.csv")
        
        if not os.path.exists(csv_path):
            print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(csv_path)
        df['ts'] = pd.to_datetime(df['ts'])
        df = df.set_index('ts')
        
        # æ—¶é—´è¿‡æ»¤
        start_date = pd.to_datetime(self.config.train_start, utc=True)
        end_date = pd.to_datetime(self.config.train_end, utc=True)
        df = df[(df.index >= start_date) & (df.index <= end_date)]
        
        return df
    
    def calculate_reward(self, returns: pd.Series, positions: pd.Series, 
                        prices: pd.Series) -> float:
        """è®¡ç®—å¥–åŠ±"""
        if self.config.reward_function == 'sharpe':
            # å¤æ™®æ¯”ç‡
            if returns.std() == 0:
                return 0.0
            return returns.mean() / returns.std() * np.sqrt(252)
        
        elif self.config.reward_function == 'return':
            # æ€»æ”¶ç›Š
            return (1 + returns).prod() - 1
        
        elif self.config.reward_function == 'risk_adjusted':
            # é£é™©è°ƒæ•´æ”¶ç›Š
            total_return = (1 + returns).prod() - 1
            
            # è®¡ç®—å›æ’¤
            cumulative = (1 + returns).cumprod()
            running_max = cumulative.expanding().max()
            drawdown = (cumulative - running_max) / running_max
            max_drawdown = abs(drawdown.min())
            
            # è®¡ç®—æ³¢åŠ¨ç‡
            volatility = returns.std() * np.sqrt(252)
            
            # é£é™©è°ƒæ•´æ”¶ç›Š
            risk_adjusted_return = total_return - (self.config.alpha_dd * max_drawdown + self.config.alpha_vol * volatility)
            
            return risk_adjusted_return
        
        else:
            return 0.0
    
    def generate_training_samples(self, df: pd.DataFrame) -> List[Tuple[np.ndarray, int, float]]:
        """ç”Ÿæˆè®­ç»ƒæ ·æœ¬"""
        samples = []
        
        if df.empty:
            return samples
        
        prices = df['close']
        features_df = build_regime_features(prices)
        
        # ç¡®ä¿featuresæ˜¯DataFrameæ ¼å¼
        if isinstance(features_df, pd.Series):
            features_df = features_df.to_frame().T
        
        # ä¸ºæ¯ä¸ªç­–ç•¥ç”Ÿæˆæ ·æœ¬
        for strategy_name in self.strategy_library.get_strategy_names():
            try:
                strategy = self.strategy_library.get_strategy(strategy_name)
                if strategy is None:
                    continue
                
                # è¿è¡Œç­–ç•¥
                output = strategy.run(prices, features_df)
                positions = output.positions
                
                # è®¡ç®—æ”¶ç›Š
                returns = prices.pct_change()
                strategy_returns = positions.shift(1) * returns
                
                # è®¡ç®—å¥–åŠ±
                reward = self.calculate_reward(strategy_returns, positions, prices)
                
                # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹ç”Ÿæˆæ ·æœ¬
                for i in range(20, len(features_df)):  # è·³è¿‡å‰20ä¸ªæ•°æ®ç‚¹
                    # è·å–æ•°å€¼ç‰¹å¾
                    numeric_features = features_df.select_dtypes(include=[np.number])
                    if len(numeric_features.columns) > 0:
                        feature_vector = numeric_features.iloc[i].values
                    if len(feature_vector) > 0:
                        arm_idx = self.strategy_library.get_strategy_names().index(strategy_name)
                        samples.append((feature_vector, arm_idx, reward))
                
            except Exception as e:
                print(f"    ç­–ç•¥ {strategy_name} æ ·æœ¬ç”Ÿæˆå¤±è´¥: {e}")
                continue
        
        return samples
    
    def train_selector(self) -> Any:
        """è®­ç»ƒé€‰æ‹©å™¨"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒé€‰æ‹©å™¨: {self.config.selector_type}")
        print(f"ğŸ“… è®­ç»ƒæ—¶é—´: {self.config.train_start} åˆ° {self.config.train_end}")
        print(f"ğŸ“Š èµ„äº§åˆ—è¡¨: {self.config.assets}")
        
        # åˆ›å»ºé€‰æ‹©å™¨
        config = ThompsonSelectorConfig(
            n_arms=len(self.strategy_library.get_strategy_names()),
            n_features=50,  # å°†æ ¹æ®å®é™…ç‰¹å¾è°ƒæ•´
            alpha=1.0,
            beta=1.0
        )
        selector = create_selector(self.config.selector_type, config)
        
        # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ ·æœ¬
        all_samples = []
        
        for asset in self.config.assets:
            print(f"\nğŸ“ˆ å¤„ç†èµ„äº§: {asset}")
            
            # åŠ è½½æ•°æ®
            df = self.load_asset_data(asset)
            if df.empty:
                print(f"    âš ï¸  è·³è¿‡ {asset}: æ— æ•°æ®")
                continue
            
            print(f"    ğŸ“Š æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
            samples = self.generate_training_samples(df)
            print(f"    ğŸ”¢ æ ·æœ¬: {len(samples)} ä¸ª")
            
            all_samples.extend(samples)
        
        print(f"\nğŸ“Š æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        
        if not all_samples:
            print("âŒ æ²¡æœ‰è®­ç»ƒæ ·æœ¬ï¼Œæ— æ³•è®­ç»ƒé€‰æ‹©å™¨")
            return None
        
        # æ›´æ–°é€‰æ‹©å™¨ç‰¹å¾ç»´åº¦
        feature_dim = len(all_samples[0][0])
        selector.n_features = feature_dim
        selector.config.n_features = feature_dim
        
        # é‡æ–°åˆå§‹åŒ–é€‰æ‹©å™¨
        selector.mu = np.zeros((selector.n_arms, feature_dim))
        selector.Sigma = np.array([np.eye(feature_dim) for _ in range(selector.n_arms)])
        selector.b = np.zeros((selector.n_arms, feature_dim))
        selector.A = np.array([np.eye(feature_dim) for _ in range(selector.n_arms)])
        
        # è®­ç»ƒé€‰æ‹©å™¨
        print(f"ğŸ”„ è®­ç»ƒé€‰æ‹©å™¨: {len(all_samples)} ä¸ªæ ·æœ¬")
        
        for i, (feature_vector, arm_idx, reward) in enumerate(all_samples):
            selector.update(feature_vector, arm_idx, reward)
            
            if (i + 1) % 1000 == 0:
                print(f"    å·²å¤„ç†: {i + 1}/{len(all_samples)} æ ·æœ¬")
        
        # æ˜¾ç¤ºè®­ç»ƒç»“æœ
        print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
        stats = selector.get_arm_stats()
        for arm, stat in stats.items():
            strategy_name = self.strategy_library.get_strategy_names()[int(arm.split('_')[1])]
            print(f"    {strategy_name}: æ‹‰åŠ¨æ¬¡æ•° {stat['n_pulls']}, å¹³å‡å¥–åŠ± {stat['avg_reward']:.4f}")
        
        return selector
    
    def save_selector(self, selector: Any):
        """ä¿å­˜é€‰æ‹©å™¨çŠ¶æ€"""
        if selector is None:
            print("âŒ é€‰æ‹©å™¨ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return
        
        try:
            selector.save_state(self.config.save_path)
            print(f"ğŸ’¾ é€‰æ‹©å™¨çŠ¶æ€å·²ä¿å­˜: {self.config.save_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def load_selector(self) -> Any:
        """åŠ è½½é€‰æ‹©å™¨çŠ¶æ€"""
        if not os.path.exists(self.config.save_path):
            print(f"âš ï¸  çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {self.config.save_path}")
            return None
        
        try:
            config = ThompsonSelectorConfig(
                n_arms=len(self.strategy_library.get_strategy_names()),
                n_features=50
            )
            selector = create_selector(self.config.selector_type, config)
            selector.load_state(self.config.save_path)
            print(f"âœ… é€‰æ‹©å™¨çŠ¶æ€å·²åŠ è½½: {self.config.save_path}")
            return selector
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def update_selector(self, new_data: pd.DataFrame, asset: str):
        """åœ¨çº¿æ›´æ–°é€‰æ‹©å™¨"""
        print(f"ğŸ”„ åœ¨çº¿æ›´æ–°é€‰æ‹©å™¨: {asset}")
        
        # åŠ è½½ç°æœ‰é€‰æ‹©å™¨
        selector = self.load_selector()
        if selector is None:
            print("âŒ æ— æ³•åŠ è½½é€‰æ‹©å™¨ï¼Œè¯·å…ˆè®­ç»ƒ")
            return
        
        # ç”Ÿæˆæ–°æ ·æœ¬
        samples = self.generate_training_samples(new_data)
        print(f"    ğŸ“Š æ–°æ ·æœ¬: {len(samples)} ä¸ª")
        
        # æ›´æ–°é€‰æ‹©å™¨
        for feature_vector, arm_idx, reward in samples:
            selector.update(feature_vector, arm_idx, reward)
        
        # ä¿å­˜æ›´æ–°åçš„çŠ¶æ€
        self.save_selector(selector)
        print(f"âœ… é€‰æ‹©å™¨å·²æ›´æ–°")
    
    def evaluate_selector(self, test_data: pd.DataFrame, asset: str) -> Dict[str, Any]:
        """è¯„ä¼°é€‰æ‹©å™¨æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼°é€‰æ‹©å™¨æ€§èƒ½: {asset}")
        
        # åŠ è½½é€‰æ‹©å™¨
        selector = self.load_selector()
        if selector is None:
            print("âŒ æ— æ³•åŠ è½½é€‰æ‹©å™¨")
            return {}
        
        prices = test_data['close']
        features = build_regime_features(prices)
        
        # æµ‹è¯•é€‰æ‹©å™¨æ¨è
        feature_vector = features.iloc[-1].select_dtypes(include=[np.number]).values
        if len(feature_vector) == 0:
            print("âŒ æ— æœ‰æ•ˆç‰¹å¾")
            return {}
        
        # è·å–æ¨èç­–ç•¥
        selected_arms = selector.pick(feature_vector, self.config.topk)
        strategy_names = self.strategy_library.get_strategy_names()
        selected_strategies = [strategy_names[arm] for arm in selected_arms]
        
        print(f"    ğŸ¯ æ¨èç­–ç•¥: {selected_strategies}")
        
        # æµ‹è¯•æ¨èç­–ç•¥
        results = {}
        for strategy_name in selected_strategies:
            try:
                strategy = self.strategy_library.get_strategy(strategy_name)
                if strategy is None:
                    continue
                
                # è¿è¡Œç­–ç•¥
                output = strategy.run(prices, features_df)
                positions = output.positions
                
                # è®¡ç®—æ”¶ç›Š
                returns = prices.pct_change()
                strategy_returns = positions.shift(1) * returns
                
                # è®¡ç®—æŒ‡æ ‡
                total_return = (1 + strategy_returns).prod() - 1
                volatility = strategy_returns.std() * np.sqrt(252)
                sharpe = total_return / volatility if volatility > 0 else 0
                
                results[strategy_name] = {
                    'total_return': total_return,
                    'volatility': volatility,
                    'sharpe_ratio': sharpe
                }
                
            except Exception as e:
                print(f"    ç­–ç•¥ {strategy_name} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        return results
    
    def run_training(self) -> Any:
        """è¿è¡Œè®­ç»ƒ"""
        print("ğŸš€ é€‰æ‹©å™¨è®­ç»ƒå™¨")
        print("=" * 60)
        
        # è®­ç»ƒé€‰æ‹©å™¨
        selector = self.train_selector()
        
        if selector is not None:
            # ä¿å­˜é€‰æ‹©å™¨
            self.save_selector(selector)
            
            # æ˜¾ç¤ºé€‰æ‹©å™¨ä¿¡æ¯
            info = selector.get_info()
            print(f"\nğŸ“Š é€‰æ‹©å™¨ä¿¡æ¯:")
            print(f"    è‡‚æ•°: {info['n_arms']}")
            print(f"    ç‰¹å¾ç»´åº¦: {info['n_features']}")
            print(f"    æ˜¯å¦åˆå§‹åŒ–: {info['is_initialized']}")
            print(f"    æ€»æ‹‰åŠ¨æ¬¡æ•°: {info['total_pulls']}")
            
            return selector
        else:
            print("âŒ è®­ç»ƒå¤±è´¥")
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é€‰æ‹©å™¨è®­ç»ƒå™¨')
    parser.add_argument('--train-start', type=str, default='2022-01-01', help='è®­ç»ƒå¼€å§‹æ—¥æœŸ')
    parser.add_argument('--train-end', type=str, default='2024-06-30', help='è®­ç»ƒç»“æŸæ—¥æœŸ')
    parser.add_argument('--selector', type=str, default='thompson', help='é€‰æ‹©å™¨ç±»å‹')
    parser.add_argument('--topk', type=int, default=1, help='é€‰æ‹©ç­–ç•¥æ•°é‡')
    parser.add_argument('--save', type=str, default='artifacts/selector_state.pkl', help='ä¿å­˜è·¯å¾„')
    parser.add_argument('--data-path', type=str, default='./data', help='æ•°æ®è·¯å¾„')
    parser.add_argument('--assets', nargs='+', default=['BTC', 'ETH', 'SOL'], help='è®­ç»ƒèµ„äº§')
    parser.add_argument('--reward-function', type=str, default='sharpe', help='å¥–åŠ±å‡½æ•°')
    parser.add_argument('--alpha-dd', type=float, default=0.5, help='å›æ’¤æƒ©ç½šç³»æ•°')
    parser.add_argument('--alpha-vol', type=float, default=0.3, help='æ³¢åŠ¨ç‡æƒ©ç½šç³»æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig(
        train_start=args.train_start,
        train_end=args.train_end,
        selector_type=args.selector,
        topk=args.topk,
        save_path=args.save,
        data_path=args.data_path,
        assets=args.assets,
        reward_function=args.reward_function,
        alpha_dd=args.alpha_dd,
        alpha_vol=args.alpha_vol
    )
    
    # è¿è¡Œè®­ç»ƒ
    trainer = SelectorTrainer(config)
    selector = trainer.run_training()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()